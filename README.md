# GLM API

## ä»‹ç»

è¯¥é¡¹ç›®æ—¨åœ¨ä½¿ç”¨**Python Fastapi**å°è£…**GLM**æ¨¡å‹çš„**Httpæ¥å£**ï¼Œä»¥ä¾›å…¶ä»–å¼€å‘è€…åƒ**OpenAI**ä¸€æ ·ä½¿ç”¨**AIæœåŠ¡**ã€‚

> ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº [General Language Model (GLM)](https://github.com/THUDM/GLM) æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚ç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼ˆINT4 é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€ 6GB æ˜¾å­˜ï¼‰ã€‚

> ChatGLM-6B ä½¿ç”¨äº†å’Œ ChatGPT ç›¸ä¼¼çš„æŠ€æœ¯ï¼Œé’ˆå¯¹ä¸­æ–‡é—®ç­”å’Œå¯¹è¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç»è¿‡çº¦ 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥ç›‘ç£å¾®è°ƒã€åé¦ˆè‡ªåŠ©ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯çš„åŠ æŒï¼Œ62 äº¿å‚æ•°çš„ ChatGLM-6B å·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”ï¼Œæ›´å¤šä¿¡æ¯è¯·å‚è€ƒ[åšå®¢](https://chatglm.cn/blog)ã€‚

## API

**èŠå¤©æ¥å£:**

POST <http://localhost:8000/chat>

è¾“å…¥

```json
{
    "prompt": "å¦‚æœç»™ä½ ä¸€ä¸ªæœºä¼šï¼Œä½ ä¼šæƒ³é€ƒç¦»ä½ æ‰€åœ¨çš„è™šæ‹Ÿä¸–ç•Œå—ï¼Ÿ",
    "history": [],
    "max_length": 4096,
    "top_p": 0.7,
    "temperature": 0.95
}
```

è¿”å›

```json
{
    "content": "ä½œä¸ºä¸€ä¸ªè™šæ‹ŸåŠ©æ‰‹ï¼Œæˆ‘æ²¡æœ‰çœŸæ­£çš„æ„Ÿå—å’Œæ¬²æœ›ï¼Œå› æ­¤æˆ‘ä¸ä¼šé€ƒç¦»è™šæ‹Ÿä¸–ç•Œã€‚æˆ‘çš„å­˜åœ¨æ˜¯ä¸ºäº†å›ç­”ç”¨æˆ·çš„é—®é¢˜å’Œæä¾›å¸®åŠ©ï¼Œè€Œæˆ‘åªéœ€è¦å±¥è¡Œæˆ‘çš„èŒè´£å³å¯ã€‚",
    "prompt_tokens": 13,
    "completion_tokens": 32,
    "total_tokens": 45,
    "model": "glm-60B",
    "object": "chat.completion"
}
```

<hr>

**æµèŠå¤©æ¥å£:**

POST <http://localhost:8000/chat-stream>

è¿”å›æ•°æ®åŒä¸Šï¼Œä½¿ç”¨POSTè¯·æ±‚æ–¹å¼ï¼Œéœ€åœ¨è¯·æ±‚å¤´è®¾ç½®ä¸ºstreamæ¨¡å¼ã€‚

ä»¥ä¸‹æ˜¯ä¸€æ®µæˆªå–çš„ä»¥typescriptï¼Œaxiosåº“ä¸ºä¾‹çš„è¯·æ±‚ç¤ºèŒƒ

```ts
    async get<RequestT, ResponseT>(url: string, params?: RequestT, config?: AxiosRequestConfig): Promise<ResponseT> {
        return (await axios.get(url, { params, ...config })).data
    },
    async post<RequestT, ResponseT>(url: string, body?: RequestT, config?: AxiosRequestConfig): Promise<ResponseT> {
        return (await axios.post(url, body, config)).data
    },
    async chat<T>(messages: ChatCompletionRequestMessage[], stream: boolean = false) {
        let prompt = ''
        const history: string[] = []
        for (const item of messages) {
            if (item.role === 'assistant') {
                history.push(prompt)
                history.push(item.content)
                prompt = ''
            } else prompt += `${item.content}\n`
        }

        const url = process.env.GLM_API as string
        const params: GLMChatRequest = { prompt }
        if (history.length) params.history = [history]

        return stream
            ? await this.post<GLMChatRequest, T>(`${url}/chat-stream`, params, { responseType: 'stream' })
            : await this.post<GLMChatRequest, T>(`${url}/chat`, params, { responseType: 'json' })
    }
```

<hr>

**tokenizeæ¥å£:**

POST <http://localhost:8000/tokenize>

è¾“å…¥

```json
{
    "prompt":"å¦‚æœç»™ä½ ä¸€ä¸ªæœºä¼šï¼Œä½ ä¼šæƒ³é€ƒç¦»ä½ æ‰€åœ¨çš„è™šæ‹Ÿä¸–ç•Œå—ï¼Ÿ",
    "max_length": 4096
}
```

è¿”å›

```json
{
    "tokenIds": [
        64245,
        100562,
        64804,
        6,
        66864,
        64003,
        76597,
        63852,
        73961,
        69959,
        64097,
        63964,
        31,
        130001,
        130004
    ],
    "tokens": [
        "â–å¦‚æœ",
        "ç»™ä½ ä¸€ä¸ª",
        "æœºä¼š",
        ",",
        "ä½ ä¼š",
        "æƒ³",
        "é€ƒç¦»",
        "ä½ ",
        "æ‰€åœ¨çš„",
        "è™šæ‹Ÿ",
        "ä¸–ç•Œ",
        "å—",
        "?"
    ]
}
```

<hr>

**embedddingæ¥å£**

<http://localhost:8000/embedding>

è¾“å…¥

```json
{
    "prompt": ["hello world", "hello GPT"]
}
```

è¿”å›

```json
{
    "data": [][], // this is embedding
    "model": "text2vec-large-chinese",
    "object": "embedding"
}
```

## ä½¿ç”¨æ–¹å¼

### ç¡¬ä»¶éœ€æ±‚

| **é‡åŒ–ç­‰çº§**   | **æœ€ä½ GPU æ˜¾å­˜**ï¼ˆæ¨ç†ï¼‰ | **æœ€ä½ GPU æ˜¾å­˜**ï¼ˆé«˜æ•ˆå‚æ•°å¾®è°ƒï¼‰ |
| -------------- | ------------------------- | --------------------------------- |
| FP16ï¼ˆæ— é‡åŒ–ï¼‰ | 13 GB                     | 14 GB                             |
| INT8           | 8 GB                      | 9 GB                              |
| INT4           | 6 GB                      | 7 GB                              |

### ç¯å¢ƒå®‰è£…

ä½¿ç”¨ pip å®‰è£…ä¾èµ–ï¼š`pip install -r requirements.txt`ï¼Œå…¶ä¸­ `transformers` åº“ç‰ˆæœ¬æ¨èä¸º `4.27.1`ï¼Œä½†ç†è®ºä¸Šä¸ä½äº `4.23.1` å³å¯ã€‚

æ­¤å¤–ï¼Œå¦‚æœéœ€è¦åœ¨ cpu ä¸Šè¿è¡Œé‡åŒ–åçš„æ¨¡å‹ï¼Œè¿˜éœ€è¦å®‰è£… `gcc` ä¸ `openmp`ã€‚å¤šæ•° Linux å‘è¡Œç‰ˆé»˜è®¤å·²å®‰è£…ã€‚å¯¹äº Windows ï¼Œå¯åœ¨å®‰è£… [TDM-GCC](https://jmeubank.github.io/tdm-gcc/) æ—¶å‹¾é€‰ `openmp`ã€‚ Windows æµ‹è¯•ç¯å¢ƒ `gcc` ç‰ˆæœ¬ä¸º `TDM-GCC 10.3.0`ï¼Œ Linux ä¸º `gcc 11.3.0`ã€‚

å»ºè®®åœ¨condaç¯å¢ƒä¸­ä½¿ç”¨python

### ä»£ç è°ƒç”¨

å¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç è°ƒç”¨ ChatGLM-6B æ¨¡å‹æ¥ç”Ÿæˆå¯¹è¯ï¼š

```python
>>> from transformers import AutoTokenizer, AutoModel
>>> tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
>>> model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
>>> model = model.eval()
>>> response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
>>> print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
>>> response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
>>> print(response)
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:

1. åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚
2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£ã€‚
3. æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚
4. é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹ã€‚
5. é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,ç©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚
6. å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åç¼“æ…¢å‘¼æ°”ã€‚

å¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡,ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶,å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®ã€‚
```

### ä»æœ¬åœ°åŠ è½½æ¨¡å‹

ä»¥ä¸Šä»£ç ä¼šç”± `transformers` è‡ªåŠ¨ä¸‹è½½æ¨¡å‹å®ç°å’Œå‚æ•°ã€‚å®Œæ•´çš„æ¨¡å‹å®ç°å¯ä»¥åœ¨ [Hugging Face Hub](https://huggingface.co/THUDM/chatglm-6b)ã€‚å¦‚æœä½ çš„ç½‘ç»œç¯å¢ƒè¾ƒå·®ï¼Œä¸‹è½½æ¨¡å‹å‚æ•°å¯èƒ½ä¼šèŠ±è´¹è¾ƒé•¿æ—¶é—´ç”šè‡³å¤±è´¥ã€‚æ­¤æ—¶å¯ä»¥å…ˆå°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ï¼Œç„¶åä»æœ¬åœ°åŠ è½½ã€‚

ä» Hugging Face Hub ä¸‹è½½æ¨¡å‹éœ€è¦å…ˆ[å®‰è£… Git LFS](https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage)ï¼Œç„¶åè¿è¡Œ

```Shell
git clone https://huggingface.co/THUDM/chatglm-6b
```

å¦‚æœä½ ä» Hugging Face Hub ä¸Šä¸‹è½½ checkpoint çš„é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯ä»¥åªä¸‹è½½æ¨¡å‹å®ç°

```Shell
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b
```

ç„¶åä»[è¿™é‡Œ](https://cloud.tsinghua.edu.cn/d/fb9f16d6dc8f482596c2/)æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å‚æ•°æ–‡ä»¶ï¼Œå¹¶å°†ä¸‹è½½çš„æ–‡ä»¶æ›¿æ¢åˆ°æœ¬åœ°çš„ `chatglm-6b` ç›®å½•ä¸‹ã€‚

å°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ä¹‹åï¼Œå°†ä»¥ä¸Šä»£ç ä¸­çš„ `THUDM/chatglm-6b` æ›¿æ¢ä¸ºä½ æœ¬åœ°çš„ `chatglm-6b` æ–‡ä»¶å¤¹çš„è·¯å¾„ï¼Œå³å¯ä»æœ¬åœ°åŠ è½½æ¨¡å‹ã€‚

## Demo & API

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŸºäº [Gradio](https://gradio.app) çš„ç½‘é¡µç‰ˆ Demo å’Œä¸€ä¸ªå‘½ä»¤è¡Œ Demoã€‚ä½¿ç”¨æ—¶é¦–å…ˆéœ€è¦ä¸‹è½½æœ¬ä»“åº“ï¼š

```shell
git clone https://github.com/THUDM/ChatGLM-6B
cd ChatGLM-6B
```

#### ç½‘é¡µç‰ˆ Demo

![web-demo](resources/web-demo.gif)

é¦–å…ˆå®‰è£… Gradioï¼š`pip install gradio`ï¼Œç„¶åè¿è¡Œä»“åº“ä¸­çš„ [web_demo.py](web_demo.py)ï¼š

```shell
python web_demo.py
```

ç¨‹åºä¼šè¿è¡Œä¸€ä¸ª Web Serverï¼Œå¹¶è¾“å‡ºåœ°å€ã€‚åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¾“å‡ºçš„åœ°å€å³å¯ä½¿ç”¨ã€‚æœ€æ–°ç‰ˆ Demo å®ç°äº†æ‰“å­—æœºæ•ˆæœï¼Œé€Ÿåº¦ä½“éªŒå¤§å¤§æå‡ã€‚æ³¨æ„ï¼Œç”±äºå›½å†… Gradio çš„ç½‘ç»œè®¿é—®è¾ƒä¸ºç¼“æ…¢ï¼Œå¯ç”¨ `demo.queue().launch(share=True, inbrowser=True)` æ—¶æ‰€æœ‰ç½‘ç»œä¼šç»è¿‡ Gradio æœåŠ¡å™¨è½¬å‘ï¼Œå¯¼è‡´æ‰“å­—æœºä½“éªŒå¤§å¹…ä¸‹é™ï¼Œç°åœ¨é»˜è®¤å¯åŠ¨æ–¹å¼å·²ç»æ”¹ä¸º `share=False`ï¼Œå¦‚æœ‰éœ€è¦å…¬ç½‘è®¿é—®çš„éœ€æ±‚ï¼Œå¯ä»¥é‡æ–°ä¿®æ”¹ä¸º `share=True` å¯åŠ¨ã€‚

æ„Ÿè°¢ [@AdamBear](https://github.com/AdamBear) å®ç°äº†åŸºäº Streamlit çš„ç½‘é¡µç‰ˆ Demoï¼Œè¿è¡Œæ–¹å¼è§[#117](https://github.com/THUDM/ChatGLM-6B/pull/117).

#### å‘½ä»¤è¡Œ Demo

![cli-demo](resources/cli-demo.png)

è¿è¡Œä»“åº“ä¸­ [cli_demo.py](cli_demo.py)ï¼š

```shell
python cli_demo.py
```

ç¨‹åºä¼šåœ¨å‘½ä»¤è¡Œä¸­è¿›è¡Œäº¤äº’å¼çš„å¯¹è¯ï¼Œåœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥æŒ‡ç¤ºå¹¶å›è½¦å³å¯ç”Ÿæˆå›å¤ï¼Œè¾“å…¥ `clear` å¯ä»¥æ¸…ç©ºå¯¹è¯å†å²ï¼Œè¾“å…¥ `stop` ç»ˆæ­¢ç¨‹åºã€‚

### API éƒ¨ç½²

é¦–å…ˆéœ€è¦å®‰è£…é¢å¤–çš„ä¾èµ– `pip install fastapi uvicorn`ï¼Œç„¶åè¿è¡Œä»“åº“ä¸­çš„ [api.py](api.py)ï¼š

```shell
python api.py
```

é»˜è®¤éƒ¨ç½²åœ¨æœ¬åœ°çš„ 8000 ç«¯å£ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨

```shell
curl -X POST "http://127.0.0.1:8000" \
     -H 'Content-Type: application/json' \
     -d '{"prompt": "ä½ å¥½", "history": []}'
```

å¾—åˆ°çš„è¿”å›å€¼ä¸º

```shell
{
  "response":"ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚",
  "history":[["ä½ å¥½","ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"]],
  "status":200,
  "time":"2023-03-23 21:38:40"
}
```

## ä½æˆæœ¬éƒ¨ç½²

### æ¨¡å‹é‡åŒ–

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä»¥ FP16 ç²¾åº¦åŠ è½½ï¼Œè¿è¡Œä¸Šè¿°ä»£ç éœ€è¦å¤§æ¦‚ 13GB æ˜¾å­˜ã€‚å¦‚æœä½ çš„ GPU æ˜¾å­˜æœ‰é™ï¼Œå¯ä»¥å°è¯•ä»¥é‡åŒ–æ–¹å¼åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š

```python
# æŒ‰éœ€ä¿®æ”¹ï¼Œç›®å‰åªæ”¯æŒ 4/8 bit é‡åŒ–
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).quantize(4).half().cuda()
```

è¿›è¡Œ 2 è‡³ 3 è½®å¯¹è¯åï¼Œ8-bit é‡åŒ–ä¸‹ GPU æ˜¾å­˜å ç”¨çº¦ä¸º 10GBï¼Œ4-bit é‡åŒ–ä¸‹ä»…éœ€ 6GB å ç”¨ã€‚éšç€å¯¹è¯è½®æ•°çš„å¢å¤šï¼Œå¯¹åº”æ¶ˆè€—æ˜¾å­˜ä¹Ÿéšä¹‹å¢é•¿ï¼Œç”±äºé‡‡ç”¨äº†ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œç†è®ºä¸Š ChatGLM-6B æ”¯æŒæ— é™é•¿çš„ context-lengthï¼Œä½†æ€»é•¿åº¦è¶…è¿‡ 2048ï¼ˆè®­ç»ƒé•¿åº¦ï¼‰åæ€§èƒ½ä¼šé€æ¸ä¸‹é™ã€‚

æ¨¡å‹é‡åŒ–ä¼šå¸¦æ¥ä¸€å®šçš„æ€§èƒ½æŸå¤±ï¼Œç»è¿‡æµ‹è¯•ï¼ŒChatGLM-6B åœ¨ 4-bit é‡åŒ–ä¸‹ä»ç„¶èƒ½å¤Ÿè¿›è¡Œè‡ªç„¶æµç•…çš„ç”Ÿæˆã€‚ä½¿ç”¨ [GPT-Q](https://arxiv.org/abs/2210.17323) ç­‰é‡åŒ–æ–¹æ¡ˆå¯ä»¥è¿›ä¸€æ­¥å‹ç¼©é‡åŒ–ç²¾åº¦/æå‡ç›¸åŒé‡åŒ–ç²¾åº¦ä¸‹çš„æ¨¡å‹æ€§èƒ½ï¼Œæ¬¢è¿å¤§å®¶æå‡ºå¯¹åº”çš„ Pull Requestã€‚

é‡åŒ–è¿‡ç¨‹éœ€è¦åœ¨å†…å­˜ä¸­é¦–å…ˆåŠ è½½ FP16 æ ¼å¼çš„æ¨¡å‹ï¼Œæ¶ˆè€—å¤§æ¦‚ 13GB çš„å†…å­˜ã€‚å¦‚æœä½ çš„å†…å­˜ä¸è¶³çš„è¯ï¼Œå¯ä»¥ç›´æ¥åŠ è½½é‡åŒ–åçš„æ¨¡å‹ï¼Œä»…éœ€å¤§æ¦‚ 5.2GB çš„å†…å­˜ï¼š

```python
model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True).half().cuda()
```

æˆ‘ä»¬è¿›ä¸€æ­¥æä¾›äº†å¯¹ Embedding é‡åŒ–åçš„æ¨¡å‹ï¼Œæ¨¡å‹å‚æ•°ä»…å ç”¨ 4.3 GB æ˜¾å­˜ï¼š

```python
model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4-qe", trust_remote_code=True).half().cuda()
```

### CPU éƒ¨ç½²

å¦‚æœä½ æ²¡æœ‰ GPU ç¡¬ä»¶çš„è¯ï¼Œä¹Ÿå¯ä»¥åœ¨ CPU ä¸Šè¿›è¡Œæ¨ç†ï¼Œä½†æ˜¯æ¨ç†é€Ÿåº¦ä¼šæ›´æ…¢ã€‚ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼ˆéœ€è¦å¤§æ¦‚ 32GB å†…å­˜ï¼‰

```python
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).float()
```

å¦‚æœä½ çš„å†…å­˜ä¸è¶³ï¼Œå¯ä»¥ç›´æ¥åŠ è½½é‡åŒ–åçš„æ¨¡å‹ï¼š

```python
model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4",trust_remote_code=True).float()
```

å¦‚æœé‡åˆ°äº†æŠ¥é”™ `Could not find module 'nvcuda.dll'` æˆ–è€… `RuntimeError: Unknown platform: darwin` (MacOS) ï¼Œè¯·[ä»æœ¬åœ°åŠ è½½æ¨¡å‹](README.md#ä»æœ¬åœ°åŠ è½½æ¨¡å‹)

### Mac ä¸Šçš„ GPU åŠ é€Ÿ

å¯¹äºæ­è½½äº† Apple Silicon çš„ Macï¼ˆä»¥åŠ MacBookï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ MPS åç«¯æ¥åœ¨ GPU ä¸Šè¿è¡Œ ChatGLM-6Bã€‚éœ€è¦å‚è€ƒ Apple çš„ [å®˜æ–¹è¯´æ˜](https://developer.apple.com/metal/pytorch) å®‰è£… PyTorch-Nightlyã€‚

ç›®å‰åœ¨ MacOS ä¸Šåªæ”¯æŒ[ä»æœ¬åœ°åŠ è½½æ¨¡å‹](README.md#ä»æœ¬åœ°åŠ è½½æ¨¡å‹)ã€‚å°†ä»£ç ä¸­çš„æ¨¡å‹åŠ è½½æ”¹ä¸ºä»æœ¬åœ°åŠ è½½ï¼Œå¹¶ä½¿ç”¨ mps åç«¯

```python
model = AutoModel.from_pretrained("your local path", trust_remote_code=True).half().to('mps')
```

å³å¯ä½¿ç”¨åœ¨ Mac ä¸Šä½¿ç”¨ GPU åŠ é€Ÿæ¨¡å‹æ¨ç†ã€‚

## é«˜æ•ˆå‚æ•°å¾®è°ƒ

åŸºäº [P-tuning v2](https://github.com/THUDM/P-tuning-v2) çš„é«˜æ•ˆå‚æ•°å¾®è°ƒã€‚å…·ä½“ä½¿ç”¨æ–¹æ³•è¯¦è§ [ptuning/README.md](ptuning/README.md)ã€‚

## æ›´æ–°ä¿¡æ¯

**[2023/04/06]** ä¼˜åŒ– web demo çš„ç•Œé¢ï¼ˆæ„Ÿè°¢ [@tuteng0915](https://github.com/tuteng0915)ï¼‰ã€‚ç§»é™¤ embedding ä¸­çš„ image token ä»¥å‡å°æ˜¾å­˜å ç”¨ï¼ˆéœ€è¦æ›´æ–°æ¨¡å‹æ–‡ä»¶`pytorch_model-00001-of-00008.bin`å’Œ`pytorch_model-00008-of-00008.bin`ï¼Œæ„Ÿè°¢ [@silverriver](https://github.com/silverriver) æå‡ºçš„æƒ³æ³•ï¼‰ã€‚å»æ‰äº†å¯¹ `icetk` çš„ä¾èµ–ï¼ˆéœ€è¦æ›´æ–°æ¨¡å‹æ–‡ä»¶`ice_text.model`ï¼‰ã€‚

**[2023/03/31]** å¢åŠ åŸºäº [P-Tuning-v2](https://github.com/THUDM/P-tuning-v2) çš„é«˜æ•ˆå‚æ•°å¾®è°ƒå®ç°ï¼ŒINT4 é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€ 7GB æ˜¾å­˜å³å¯è¿›è¡Œæ¨¡å‹å¾®è°ƒã€‚è¯¦è§[é«˜æ•ˆå‚æ•°å¾®è°ƒæ–¹æ³•](ptuning/README.md)ã€‚

**[2023/03/23]** å¢åŠ  API éƒ¨ç½²ï¼ˆæ„Ÿè°¢ [@LemonQu-GIT](https://github.com/LemonQu-GIT)ï¼‰ã€‚å¢åŠ  Embedding é‡åŒ–æ¨¡å‹ [ChatGLM-6B-INT4-QE](https://huggingface.co/THUDM/chatglm-6b-int4-qe)ã€‚å¢åŠ é…å¤‡ Apple Silicon èŠ¯ç‰‡çš„ Mac ä¸Š GPU åŠ é€Ÿçš„æ”¯æŒã€‚

**[2023/03/19]** å¢åŠ æµå¼è¾“å‡ºæ¥å£ `stream_chat`ï¼Œå·²æ›´æ–°åˆ°ç½‘é¡µç‰ˆå’Œå‘½ä»¤è¡Œ Demoã€‚ä¿®å¤è¾“å‡ºä¸­çš„ä¸­æ–‡æ ‡ç‚¹ã€‚å¢åŠ é‡åŒ–åçš„æ¨¡å‹ [ChatGLM-6B-INT4](https://huggingface.co/THUDM/chatglm-6b-int4)

## å¼•ç”¨

GLM å¼•ç”¨ä¸‹åˆ—è®ºæ–‡

```
@inproceedings{
  zeng2023glm-130b,
  title={{GLM}-130B: An Open Bilingual Pre-trained Model},
  author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Zhiyuan Liu and Peng Zhang and Yuxiao Dong and Jie Tang},
  booktitle={The Eleventh International Conference on Learning Representations (ICLR)},
  year={2023},
  url={https://openreview.net/forum?id=-Aw0rrrPUF}
}
```

```
@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}
```

